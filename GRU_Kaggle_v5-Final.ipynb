{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"scrolled":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\n#Importing libraries:\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport nltk\nimport nltk.data\nimport re\nfrom bs4 import BeautifulSoup\nfrom nltk.tokenize import WhitespaceTokenizer\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nfrom nltk.corpus import wordnet\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\ndf = pd.read_excel('../input/blog-data/final_data_17406.xlsx')\ndf.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"da4716464b06b3a76877ba7cc3f0fc728409e1bc"},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore', category=FutureWarning)\n#df.drop(['id','date','img_src','section','topics','url'],axis=1,inplace=True)\ndf_main = df[['content','title','startups','mobile','apps','social','gadgets','europe','enterprise']]\ndf_main.dropna(inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e9520244e01e3478483a16f56833c355242db45b"},"cell_type":"code","source":"df_main['content'] = df_main['content'].str.lstrip()\ndf_main['title'] = df_main['title'].str.lstrip()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2f8545154a82d36aa3d83319677532d924853a8d"},"cell_type":"code","source":"def checking_Data(index):\n    row = df_main[df_main.index == index][['title', 'content']].values[0]\n    if len(row) > 0:\n        print('Title:',row[0])\n        print('Content:',row[1])        \nchecking_Data(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"226bc2fb14dab9e37292223062db84b0f84d0c47"},"cell_type":"code","source":"#Cleansing Content and Tags:\nreplace_blank_regex = re.compile('[/(){}\\[\\]\\|@,;]')\nsymbols_removel_regex = re.compile('[^0-9a-z #+_$%-.,]')\nstopword_det = set(stopwords.words('english'))\n\ndef clean_text(text):\n    text = BeautifulSoup(text, \"lxml\").text # HTML decoding\n    text = text.lower() # lowercase text\n    text = replace_blank_regex.sub(' ', text) # replace replace_blank_regex symbols by space in text\n    text = symbols_removel_regex.sub('', text) # delete symbols which are in symbols_removel_regex from text\n    #text = ' '.join(word for word in text.split() if word not in stopword_det) # delete stopwors from text\n    return text\n    \ndf_main['content'] = df_main['content'].apply(clean_text)\ndf_main['title'] = df_main['title'].apply(clean_text)\nchecking_Data(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ac546b9eabacb88b4cd8d75a48145de3b5978fc9"},"cell_type":"code","source":"#Lemmatizing Content and Title:\nw_tokenizer = nltk.tokenize.WhitespaceTokenizer()\nlemmatizer = nltk.stem.WordNetLemmatizer()\n\ndef lemmatize_text(text):\n    lem_sentence=[]\n    lem_sentence= [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]\n\n    return \" \".join(lem_sentence) \n    \ndf_main['content_lemmatized'] = df_main.content.apply(lemmatize_text)\ndf_main['title_lemmatized'] = df_main.title.apply(lemmatize_text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Combining Content+Title:\ndf_main['content+title'] = df_main['title_lemmatized']+' '+df_main['content_lemmatized']\ndf_main['content+title'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport keras\nfrom keras import optimizers\nfrom keras import backend as K\nfrom keras import regularizers\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Dropout, GlobalMaxPooling1D,SpatialDropout1D\nfrom keras.layers import Embedding, GRU, Bidirectional\nfrom keras.utils import plot_model\nfrom keras.preprocessing import sequence\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.callbacks import EarlyStopping,ModelCheckpoint,ReduceLROnPlateau\n\nfrom tqdm import tqdm\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import RegexpTokenizer \nimport os, re, csv, math, codecs\n\nsns.set_style(\"whitegrid\")\nnp.random.seed(0)\n\nDATA_PATH = '../input/'\nEMBEDDING_DIR = '../input/'\n\n\nMAX_NB_WORDS = 100000\ntokenizer = RegexpTokenizer(r'\\w+')\nstop_words = set(stopwords.words('english'))\nstop_words.update(['.', ',', '\"', \"'\", ':', ';', '(', ')', '[', ']', '{', '}'])\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#load embeddings\nprint('loading word embeddings...')\nembeddings_index = {}\nf = codecs.open('../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec', encoding='utf-8')\nfor line in tqdm(f):\n    values = line.rstrip().rsplit(' ')\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()\nprint('found %s word vectors' % len(embeddings_index))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#New Code for reading data:\n#load data\ntrain_df = df_main\nprint(\"num train: \", train_df.shape[0])\n\n\nlabel_names = ['startups','mobile','apps','social','gadgets','europe','enterprise']\ny_train = train_df[label_names].values\n\n#visualize word distribution for Content+Title:\ntrain_df['doc_len_content'] = train_df['content+title'].apply(lambda words: len(words.split(\" \")))\nmax_seq_len = np.round(train_df['doc_len_content'].mean() + train_df['doc_len_content'].std()).astype(int)\nsns.distplot(train_df['doc_len_content'], hist=True,kde=True, \n             color='b', label='doc len content')\nplt.axvline(x=max_seq_len, color='k', linestyle='--', label='max len')\nplt.title('Content+Title length'); plt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(train_df, y_train, test_size=0.2, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#New Code for tokenizing \"Content\":\nfrom keras.preprocessing.text import Tokenizer\ntokenizer = RegexpTokenizer(r'\\w+')\nraw_docs_train = X_train['content+title'].tolist()\nraw_docs_test = X_test['content+title'].tolist() \n \nnum_classes = len(label_names)\n\nprint(\"pre-processing train data...\")\nprocessed_docs_train = []\nfor doc in tqdm(raw_docs_train):\n    tokens = tokenizer.tokenize(doc)\n    filtered = [word for word in tokens if word not in stop_words]\n    processed_docs_train.append(\" \".join(filtered))\n#end for\n\nprint(\"pre-processing test data...\")\nprocessed_docs_test = []\nfor doc in tqdm(raw_docs_test):\n    tokens = tokenizer.tokenize(doc)\n    filtered = [word for word in tokens if word not in stop_words]\n    processed_docs_test.append(\" \".join(filtered))\n#end for\n\nprint(\"tokenizing input data...\")\ntokenizer = Tokenizer(num_words=MAX_NB_WORDS, lower=True, char_level=False)\ntokenizer.fit_on_texts(processed_docs_train + processed_docs_test)#leaky\nword_seq_train = tokenizer.texts_to_sequences(processed_docs_train)\nword_seq_test = tokenizer.texts_to_sequences(processed_docs_test)\nword_index = tokenizer.word_index\nprint(\"dictionary size: \", len(word_index))\n\n#pad sequences\nword_seq_train = sequence.pad_sequences(word_seq_train, maxlen=max_seq_len)\nword_seq_test = sequence.pad_sequences(word_seq_test, maxlen=max_seq_len)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_seq_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#training params\nbatch_size = 256\nnum_epochs = 10\n\n#model parameters\nnum_filters = 64 \nembed_dim = 300 \nweight_decay = 1e-4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#embedding matrix for content:\nprint('preparing embedding matrix...')\nwords_not_found = []\nnb_words = min(MAX_NB_WORDS, len(word_index))\nembedding_matrix = np.zeros((nb_words, embed_dim))\nfor word, i in word_index.items():\n    if i >= nb_words:\n        continue\n    embedding_vector = embeddings_index.get(word)\n    if (embedding_vector is not None) and len(embedding_vector) > 0:\n        # words not found in embedding index will be all-zeros.\n        embedding_matrix[i] = embedding_vector\n    else:\n        words_not_found.append(word)\nprint('number of null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_matrix.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"sample words not found: \", np.random.choice(words_not_found, 10))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#RNN architecture\n\nprint(\"training RNN-GRU ...\")\nmodel = Sequential()\nmodel.add(Embedding(nb_words, embed_dim,\n          weights=[embedding_matrix], input_length=max_seq_len, trainable=False))\nmodel.add(SpatialDropout1D(0.20))\nmodel.add(Bidirectional(GRU(256, return_sequences=True, dropout=0.20, recurrent_dropout=0.1)))\nmodel.add(GlobalMaxPooling1D())\nmodel.add(Dropout(0.20))\nmodel.add(Dense(64, activation='relu', kernel_regularizer=regularizers.l2(weight_decay)))\nmodel.add(Dropout(0.20))\nmodel.add(Dense(7, activation='sigmoid'))\nadam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy','categorical_accuracy'])\n\nearlyStopping = EarlyStopping(monitor='val_loss', patience=4, verbose=0, mode='min')\nmcp_save = ModelCheckpoint('best_model.hdf5', save_best_only=True, monitor='val_loss', mode='min')\nreduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2, verbose=1, epsilon=1e-4, mode='min')\n\nhistory = model.fit(word_seq_train, Y_train, validation_split=0.1, batch_size=128, \n                    epochs=10,callbacks=[earlyStopping,mcp_save,reduce_lr_loss])\nmodel.summary()\n\nfrom keras.models import load_model\nmodel.save('Final_model.h5')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.load_weights('best_model.hdf5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import load_model\nMod_Fin =load_model('../input/ece-657a/Final_model.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = Mod_Fin.predict(word_seq_test, batch_size=256, verbose=1)\ny_pred = pd.DataFrame(y_pred)\ny_pred = y_pred.apply(lambda x: [0 if y <= 0.5 else 1 for y in x])\ny_pred.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"###############################################################\n#import os\n#print(os.listdir(\"../input/gru-dropout-model\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#from keras.models import load_model\n#model = load_model('../input/model-keras/my_model.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Loss plots\nplt.figure()\nplt.plot(history.history['loss'], lw=2.0, color='b', label='train')\nplt.plot(history.history['val_loss'], lw=2.0, color='r', label='val')\nplt.title('GRU Training vs Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Cross-Entropy Loss')\nplt.legend(loc='upper right')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Categorical Accuracy plot:\nplt.figure()\nplt.plot(history.history['categorical_accuracy'], lw=2.0, color='b', label='train')\nplt.plot(history.history['val_categorical_accuracy'], lw=2.0, color='r', label='val')\nplt.title('GRU Training vs Validation Categorical Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Categorical Accuracy')\nplt.legend(loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Accuracy plot:\nplt.figure()\nplt.plot(history.history['acc'], lw=2.0, color='b', label='train')\nplt.plot(history.history['val_acc'], lw=2.0, color='r', label='val')\nplt.title('GRU Training vs Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend(loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Predicting:\ny_pred = model.predict(word_seq_test, batch_size=256, verbose=1)\ny_pred = pd.DataFrame(y_pred)\ny_pred = y_pred.apply(lambda x: [0 if y <= 0.5 else 1 for y in x])\ny_pred.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report,accuracy_score\nprint(\"classification_report\",classification_report(Y_test,y_pred))\nprint(\"accuracy_score\",accuracy_score(Y_test,y_pred))\nfrom sklearn.metrics import hamming_loss\nprint(\"hamming_loss\",hamming_loss(Y_test,y_pred))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Confusion Matrix:\nfrom scipy.sparse import csc_matrix\nnew_output = csc_matrix(y_pred,dtype=np.int8).toarray()\nnew_output\n\nfrom sklearn.metrics import confusion_matrix\nAcc = []\nfor i in range(1,8):\n  cm_i = confusion_matrix(Y_test[:,i-1],new_output[:,i-1])\n  Acc_i = (cm_i[0,0]+cm_i[1,1])/(cm_i[0,0]+cm_i[0,1]+cm_i[1,0]+cm_i[1,1])\n  Acc.append(Acc_i)\n  \nprint(\"Average Accuracy =\", round((sum(Acc)/len(Acc)), 2))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}